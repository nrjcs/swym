{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "swym_mnist_conv.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrjcs/swym/blob/master/swym_mnist_conv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_kjTsII0qfuL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#....................................................................#\n",
        "#  keras mnist digit classification with convolution neural network  #\n",
        "#....................................................................#\n",
        "\n",
        "\n",
        "#Load the dataset => as in the case of FCNN\n",
        "# -- Keras provides in-built support to many datasets\n",
        "# -- such as MNIST (Modified National Institute of Standards and Technology database) @ http://yann.lecun.com/exdb/mnist/\n",
        "\t# database of handwritten digits\n",
        "\t# used  extensively in optical character recognition and machine learning research\n",
        "\t# training set of 60,000 examples, and a test set of 10,000 examples\n",
        "\t# digits have been size-normalized and centered in a fixed-size image\n",
        "\t# black and white digits\n",
        "\t# 28 x 28  pixels\n",
        "\t# Keras provides method to load MNIST data set\n",
        "  \n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data() \t#Keras function\n",
        "\n",
        "print (\"mnist data downloaded...\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1PetdwiQsDjq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Print shape of dataset..it will print three tuples, namely the no. of images in dataset, height and width(60000, 28, 28)\n",
        "\n",
        "print (X_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3CwHiVn9sonK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot images...subplot function is being used...nice documentation is available on the official webpage of matplotlib\n",
        "# arguments to subplot functions are number of rows, number of columns and number of subplots in the plot...comma is mandatory if values are less than 10\n",
        "# you can experiment\n",
        "# uncomment if do not want to print\n",
        "plt.subplot(221)\t\n",
        "plt.imshow(X_train[50], cmap=plt.get_cmap('gray')) # ploting first image of training data set\n",
        "plt.subplot(222)\n",
        "plt.imshow(X_train[1304], cmap=plt.get_cmap('gray'))\t# ploting 135th image in training data set\n",
        "plt.subplot(223)\n",
        "plt.imshow(X_test[244], cmap=plt.get_cmap('gray'))\t# ploting 2445th image of test date set\n",
        "plt.subplot(224)\n",
        "plt.imshow(X_test[39], cmap=plt.get_cmap('gray'))\t# ploting 4th image of test data set\n",
        "# show the plot\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2xwxGQy2sFIc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# defining some parameters\n",
        "img_rows, img_cols = 28,28\n",
        "\n",
        "# data preprocessing\n",
        "#reshaping the data...Normalize images\n",
        "X_train = X_train.reshape(60000,28,28,1)\n",
        "X_test = X_test.reshape(10000,28,28,1)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('x_test shape:', X_test.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "\n",
        "import keras.utils\n",
        "#Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QmOvzxYp6MyI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for reference reproducing from https://keras.io/layers/convolutional/\n",
        "\n",
        "# keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', \n",
        "    # activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', ..... )\n",
        "  \n",
        "# creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. \n",
        "# If use_bias is True, a bias vector is created and added to the outputs. \n",
        "# Finally, if activation is not None, it is applied to the outputs as well.\n",
        "\n",
        "# if first layer, provide the keyword argument input_shape e.g. input_shape=(128, 128, 3) for 128x128 RGB\n",
        "\n",
        "# filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
        "# kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. \n",
        "    # Can be a single integer to specify the same value for all spatial dimensions.\n",
        "# strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. \n",
        "    # Can be a single integer to specify the same value for all spatial dimensions.\n",
        "# padding: one of \"valid\" or \"same\" (case-insensitive).\n",
        "# activation: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
        "# use_bias: Boolean, whether the layer uses a bias vector.\n",
        "# kernel_initializer: Initializer for the kernel weights matrix.\n",
        "# bias_initializer: Initializer for the bias vector.\n",
        "\n",
        "## filters and kernel_size parameters are compulsory\n",
        "\n",
        "\n",
        "\n",
        "# MaxPooling2D reproduced from https://keras.io/layers/pooling/\n",
        "# keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n",
        "\n",
        "# pool_size: integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). \n",
        "      # If only one integer is specified, the same window length will be used for both dimensions.\n",
        "# strides: Integer, tuple of 2 integers, or None. Strides values. If None, it will default to pool_size.\n",
        "# padding: One of \"valid\" or \"same\"\n",
        "# data_format: A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs.  channels_last\n",
        "        # corresponds to inputs with shape  (batch, height, width, channels) while channels_first corresponds to inputs with shape  \n",
        "        # (batch, channels, height, width). It defaults to the image_data_format value found in your Keras config file at \n",
        "        # ~/.keras/keras.json. If you never set it, then it will be \"channels_last\".\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FtPgp5lStzjk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define model architecture\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "#arch 1\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten()) #Flattens the input\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#arch 2\n",
        "#model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "#model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#model.add(Dropout(0.25))\n",
        "#model.add(Flatten()) # Flattens the input\n",
        "#model.add(Dense(128, activation='relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "#model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "print (\"keep going...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kl0RQYZyuHE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# compiling model\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
        "\n",
        "print (\"compile successful...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dRCZOEUmuKCD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs = 2\n",
        "\n",
        "# taining the network\n",
        "\n",
        "history=model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)\n",
        "\n",
        "print (\"training done...\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lEmU2ZbBuT5M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# evaluating the model\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('loss:', score[0])\n",
        "print('accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "npPQMD752vA9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LdB0OsTOeDga",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YLuTzBk7-K3w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Improving Performance\n",
        " \n",
        " > Different architecture provided in code\n",
        " \n",
        ">  Number of layers\n",
        "\n",
        "> Different layers: dropout etc.\n",
        "\n",
        "> Different hperparameters: number of filters, stride, and padding\n",
        "\n",
        "> Different learning rate for optimizer\n",
        "\n",
        "> batch size\n",
        "\n",
        "> with different optimizers\n",
        "\n",
        "> with more number of epochs\n",
        "\n",
        "> Controlling the optimizer learning rate\n",
        "\n",
        "> Increasing the size of batch computation\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}