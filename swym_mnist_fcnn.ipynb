{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "swym_mnist_fcnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrjcs/swym/blob/master/swym_mnist_fcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "P2IrTN4gCcew",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#basic fully connected neural network for mnist digit classification\n",
        "\n",
        "## nice documentation is available at https://keras.io/\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "70EjjPgoKkB9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## -- Keras provides in-built support to many datasets\n",
        "## -- such as MNIST (Modified National Institute of Standards and Technology database) @ http://yann.lecun.com/exdb/mnist/\n",
        "\t# database of handwritten digits\n",
        "\t# used  extensively in optical character recognition and machine learning research\n",
        "\t# training set of 60,000 examples, and a test set of 10,000 examples\n",
        "\t# digits have been size-normalized and centered in a fixed-size image\n",
        "\t# black and white digits\n",
        "\t# 28 x 28  pixels\n",
        "\t# Keras provides method to load MNIST data set"
      ]
    },
    {
      "metadata": {
        "id": "tKWVb27vAeL7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load MNIST data set\n",
        "from keras.datasets import mnist\t \t#dataset\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data() \t#Keras function\n",
        "\n",
        "print (\"mnist data downloaded...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TfwO2jCdAghp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Print shape of dataset..it will print three tuples, namely the no. of images in dataset, height and width(60000, 28, 28)\n",
        "\n",
        "print (X_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0I33vNMnK1TZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### plot images...\n",
        "\n",
        "> subplot function is being used\n",
        "\n",
        "> nice documentation is available on the official webpage of matplotlib\n",
        "\n",
        "> arguments to subplot functions are number of rows, number of columns and number of subplots in the plot\n",
        "\n",
        "> comma is mandatory if values are less than 10\n"
      ]
    },
    {
      "metadata": {
        "id": "PuUhUPFTAjbq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# comment if do not want to print\n",
        "\n",
        "import matplotlib.pyplot as plt\t\t\t#to plot images\n",
        "\n",
        "plt.subplot(221)\t\n",
        "plt.imshow(X_train[50], cmap=plt.get_cmap('gray')) # ploting first image of training data set\n",
        "plt.subplot(222)\n",
        "plt.imshow(X_train[1304], cmap=plt.get_cmap('gray'))\t# ploting 135th image in training data set\n",
        "plt.subplot(223)\n",
        "plt.imshow(X_test[244], cmap=plt.get_cmap('gray'))\t# ploting 2445th image of test date set\n",
        "plt.subplot(224)\n",
        "plt.imshow(X_test[39], cmap=plt.get_cmap('gray'))\t# ploting 4th image of test data set\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c4Uz0kTQAmtB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Preprocess input data for Keras\n",
        "\n",
        "# flatten 28*28 images to a 784 vector for each image and pixel precision set to 32 bit\n",
        "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
        "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# Step 4: Preprocess class labels\n",
        "# check shape of our class label data\n",
        "\n",
        "print (y_train.shape)\n",
        "#We should have 10 different classes, one for each digit, but it looks like we only have a 1-dimensional array.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eU5LFemhAsiA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#check labels for the first 10 training samples:\n",
        "print (y_train[:10])\n",
        "# output of the form [5 0 4 1 9 2 1 3 1 4]\n",
        "#The y_train and y_test data are not split into 10 distinct class labels, but rather are represented as a single array with the class values."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8MKhIS3vAuW9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.utils import np_utils\t\t#for transforming data \n",
        "\n",
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# check again\t\n",
        "print (y_train.shape)\n",
        "# (60000, 10)\n",
        "print (y_train[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KDjnsR5vLdqL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### a very simple model is being created in next few lines...this is the most important part => creating a good network\n",
        "\n",
        "### Use sequential model\n",
        ">  a Sequential model is declared as\n",
        ">>        model = Sequential()\n",
        "then dense layers are added\n",
        "\n",
        "\n",
        "> Dense implements the operation: output = activation(dot(input, kernel) + bias) \n",
        "           >>  activation is the element-wise activation function passed as the activation argument\n",
        "           >>  kernel is a weights matrix created by the layer\n",
        "           >>  bias is a bias vector created by the layer (only applicable if use_bias is True)\n",
        "      \n",
        "> adding layers (can be combined with layer declaration as well)\n",
        ">>         model = Sequential([Dense(32, input_shape=(784,)), Activation('relu'),Dense(10), Activation('softmax'),])\n",
        " \n",
        "\n",
        ">> > Or\n",
        "\n",
        ">>         model.add(Dense(32, input_dim=784))\n",
        ">>         model.add(Activation('relu'))\n",
        "\n",
        "> model needs to know what input shape it should expect\n",
        "\n",
        ">> first layer in a Sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape\n",
        "\n",
        "\n",
        "> Dense(32, input_dim=784) specifies that it is \n",
        "\t\t>> first (input) layer\n",
        "        \n",
        "  >> output dimension is 32 ($1^{st}$ argument \n",
        "    \n",
        ">> input dimension is 784\n",
        "\n",
        "   >> kernel_initializer: Initializations define the way to set the initial random weights of Keras layers.\n",
        "    \n",
        "   >> kernel_initializer='normal': name of initialization function for the weights of the layer. normal for values \n",
        "    \n",
        "   >> randomly drawn from normal distribution.\n",
        "   \n",
        "   >> many more intializers: Zeros, Ones, normal, Constant, normal , and many more\n",
        "    \n",
        "   >> If no activation function specified, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
        "\n",
        "\n",
        "> activations: Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers:    \n",
        ">> many activation function are available in Keras: relu, softmax, sigmoid, tanh, so on"
      ]
    },
    {
      "metadata": {
        "id": "yKcp5iXTAyyZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define model architecture\n",
        "\n",
        "from keras.models import Sequential\t\t#model\n",
        "from keras.layers import Dense\t\t\t#layer\n",
        "from keras.layers import Dropout\t\t#layer\n",
        "from keras import initializers      # for importing initializers of keras\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer=initializers.RandomNormal(), activation='relu')) #only one hidden layer with relu as activation function\n",
        "#model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer=initializers.Constant(value=5), activation='relu')) #only one hidden layer with relu as activation function\t\n",
        "model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\t\t\t\t\t#output layer with softmax as activation function\n",
        "\n",
        "\n",
        "#print(model.summary())\n",
        "\n",
        "print (\"congrts model defined...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZOI0ZQSM-Nv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Before training, configure the learning process, using compile() method. Three argements:\n",
        "    > loss function: the objective function that model try to minimize\n",
        "          >> many more: categorical_crossentropy, mean_squared_error, mean_squared_logarithmic_error, ......\n",
        "    > optimizer: ANN training process is an optimization task with the aim of finding a set of weights to minimize some \n",
        "    > objective function\n",
        "          >> determine how weights are updated\n",
        "          >> many more: adam (Adaptive moment estimation), sgd (Stochastic gradient descent), \n",
        "    > list of metrics: used to judge performance of model, similar to objective function however not used for training purpose\n",
        "      \n",
        "### optimizer, loss function, meterics => very important step which will determine the performance of your network"
      ]
    },
    {
      "metadata": {
        "id": "pFg3NWWpA04y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "print (\"Compilation done ...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rE4bnbsYNSM4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> training, validation, test\n",
        "\n",
        "> multiple variations of the model is trained on the training dataset => parameter tuning\n",
        "\n",
        "> one of the variation is chosen based performance on the validation set => model selection\n",
        "\n",
        "> evaluation on test set\n",
        "\n",
        "> epoch: number of times learning algorithm sees entire data\n",
        "\n",
        "> batch size: number of samples processed before updating weights\n",
        "\n",
        "> By setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch. (no information, animated bar, numbe of epochs) )"
      ]
    },
    {
      "metadata": {
        "id": "cSh9LnquA22y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Train model\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train,validation_split=0.2, epochs=2, batch_size=200, verbose=1)\n",
        "\n",
        "\n",
        "print (\"parameter tuning done...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q_u0OQ53A4_x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 8: Evaluate model\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print(\"Error: %.2f%%\" % (100-scores[1]*100))\n",
        "\n",
        "#print(model.metrics_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5TTXar8dFT7Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print (model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IyVr7wpzBAMt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Additional**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "=> Improving Performance of Simple Network: additional hidden layers (add one more dense layer)\n",
        " \n",
        "     model.add(Dense(num_classes, kernel_initializer='normal', activation='relu'))\n",
        "     \n",
        "=> Improving Performance of Simple Network: additional hidden layers (add one more dense layer)\n",
        "\n",
        "     model.add(Dense(num_classes, kernel_initializer='normal', activation='tanh'))\n",
        "\n",
        "=> Improving Performance of Simple Network: introducing dropout layer\n",
        "\n",
        "      model.add(Dropout(0.2))\n",
        "\n",
        "=> Improving Performance of Simple Network: using different optimizers: SGD, Adagrad,Adam...\n",
        "\n",
        "=> Improving Performance of Simple Network: training for more number of epochs (20)\n",
        "\n",
        "=> other options to explore\n",
        "\n",
        "\n",
        "> different learning rate for optimizer\n",
        "\n",
        "> number of neurons in hidden layer\n",
        "\n",
        "> batch size\n",
        "\n",
        "> with different optimizers\n",
        "   \n",
        "> Increasing the number of internal hidden neurons\n",
        "   \n",
        "  \n",
        "=> steps to follow to make an efficient image classifier?\n",
        "     \n",
        "     >lot of experimentation and testing to get the optimal structure and parameters"
      ]
    }
  ]
}