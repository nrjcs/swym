{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kolkata_cifar.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrjcs/swym/blob/master/swym_cifar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nuRyc46PvnrS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "cifar10 dataset:\n",
        "\n",
        "Dataset of 50,000 32x32 color training images, labeled over 10/100 categories, and 10,000 test images.\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "p12DKH5Mvm86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Flatten\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "#define parameters\n",
        "batch_size=32\n",
        "nb_epochs=2\n",
        "kernel_size=3\n",
        "pool_size=2\n",
        "conv_depth_1=32\n",
        "conv_depth_2=64\n",
        "drop_prob_1=.5\n",
        "drop_prob_2=.5\n",
        "hidden_layer_size=512\n",
        "\n",
        "#loading data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "print (X_train.shape)\n",
        "\n",
        "#preprocessing\n",
        "num_train, height, width, depth = X_train.shape\n",
        "num_classes = np.unique(y_train).shape[0]\n",
        "print (num_classes)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= np.max(X_train)\n",
        "X_test /= np.max(X_test)\n",
        "Y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "print (Y_train.shape)\n",
        "print (Y_test.shape)\n",
        "\n",
        "inp = Input(shape=(height, width, depth))\n",
        "print (inp.shape)\n",
        "\n",
        "#define model\n",
        "conv_1 = Convolution2D(conv_depth_1, (kernel_size, kernel_size), padding='same', activation='relu')(inp)\n",
        "conv_2 = Convolution2D(conv_depth_1, (kernel_size, kernel_size), padding='same', activation='relu')(conv_1)\n",
        "pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
        "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
        "conv_3 = Convolution2D(conv_depth_2, (kernel_size, kernel_size), padding='same', activation='relu')(drop_1)\n",
        "conv_4 = Convolution2D(conv_depth_2, (kernel_size, kernel_size), padding='same', activation='relu')(conv_3)\n",
        "pool_2 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_4)\n",
        "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
        "flat = Flatten()(drop_2)\n",
        "hidden = Dense(hidden_layer_size, activation='relu')(flat)\n",
        "drop_3 = Dropout(drop_prob_2)(hidden)\n",
        "out = Dense(num_classes, activation='softmax')(drop_3)\n",
        "\n",
        "#create....another way\n",
        "model = Model(inputs=inp, outputs=out)\n",
        "\n",
        "# compile\n",
        "print (\"compile now..\")\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "print(\"compilation done\")\n",
        "\n",
        "# train\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epochs,verbose=0, validation_split=0.1)\n",
        "\n",
        "\n",
        "# evaluate\n",
        "model.evaluate(X_test, Y_test, verbose=1)\n",
        "\n",
        "\n",
        "# save\n",
        "model.save('cifartest_1.h5')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l123qKhUg5Cm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reproduced from https://keras.io/getting-started/faq/#savingloading-only-a-models-weights\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "How can I save a Keras model?\n",
        "\n",
        "Saving/loading whole models (architecture + weights + optimizer state)\n",
        "\n",
        ">It is not recommended to use pickle or cPickle to save a Keras model.\n",
        "\n",
        ">You can use model.save(filepath) to save a Keras model into a single HDF5 file which will contain:\n",
        "\n",
        ">>the architecture of the model, allowing to re-create the model\n",
        "\n",
        ">>the weights of the model\n",
        "\n",
        ">>the training configuration (loss, optimizer)\n",
        "\n",
        ">>the state of the optimizer, allowing to resume training exactly where you left off.\n",
        "\n",
        ">You can then use keras.models.load_model(filepath) to reinstantiate your model. load_model will also take care of compiling the model using the saved training configuration (unless the model was never compiled in the first place).\n",
        "\n",
        "Saving/loading only a model's architecture\n",
        "\n",
        "Save the weights of a model\n",
        "\n",
        "Load the weights into a different architecture"
      ]
    }
  ]
}